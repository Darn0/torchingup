{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as tr\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.tensor as tensor\n",
    "EPS = 1e-8\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "\n",
    "    def __init__(self):\n",
    "        # for evaluation at end of epoch\n",
    "        self.reset_epoch()\n",
    "        # for storing an episode\n",
    "        self.reset_episode()\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.ep_o, self.ep_a,self.ep_r,self.ep_v = [],[],[],[]\n",
    "        self.ep_l = 0\n",
    "\n",
    "    def reset_epoch(self):\n",
    "        self.obs_buf, self.acts_buf,self.advs_buf,self.rtgs_buf = [] ,[],[],[]\n",
    "        self.logp_prev = None\n",
    "\n",
    "\n",
    "    def store_batch(self,ep_obs,ep_acts,ep_advs,ep_rtgs):\n",
    "        # when episode is over, appends episode vals to batch\n",
    "        self.obs_buf += ep_obs\n",
    "        self.acts_buf += ep_acts\n",
    "        self.advs_buf += ep_advs\n",
    "        self.rtgs_buf += ep_rtgs\n",
    "\n",
    "    def get_batch(self):\n",
    "\n",
    "        b_a, b_o = np.array(self.acts_buf).reshape(-1), np.array(self.obs_buf)\n",
    "        # important: for continuous action space reshape acts to [batch_size,1]\n",
    "        b_a = b_a.reshape(-1,1)\n",
    "        # normalize trick\n",
    "        b_adv  = np.array((self.advs_buf - np.mean(self.advs_buf))/(np.std(self.advs_buf) + 1e-8))\n",
    "        b_rtg = np.array(self.rtgs_buf)\n",
    "\n",
    "        return [b_o,b_a,b_adv,b_rtg]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.obs_buf)\n",
    "\n",
    "    def store_episode(self,o,a,r,v):\n",
    "        self.ep_o.append(o)\n",
    "        self.ep_a.append(a)\n",
    "        self.ep_r.append(r)\n",
    "        self.ep_v.append(v)\n",
    "        self.ep_l+=1\n",
    "\n",
    "    def get_episode(self):\n",
    "        return self.ep_o,self.ep_a,self.ep_r,self.ep_v,self.ep_l\n",
    "\n",
    "    \n",
    "\n",
    "class Logger:\n",
    "    \"\"\"\n",
    "    Logs relevant values and prints them\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset_logger()\n",
    "\n",
    "    def reset_logger(self):\n",
    "        self.train_r, self.ep_len = [],[]\n",
    "\n",
    "    def store(self, train_r=None, ep_len=None, train=True):\n",
    "        if train:\n",
    "            self.train_r.append(train_r)\n",
    "            self.ep_len.append(ep_len)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def get_vals(self):\n",
    "\n",
    "        vals = np.round([np.mean(self.train_r),np.mean(self.ep_len)],2)\n",
    "        return vals\n",
    "\n",
    "\n",
    "    def print_epoch(self, epoch, loss1, loss2):\n",
    "        train_r, ep_len = self.get_vals()\n",
    "\n",
    "        print('epoch {0}  pi_loss {1:.3f}  v_loss {2:.3f}  episode length {3}  returns {4}'.format(epoch,loss1,loss2, ep_len,train_r ))\n",
    "        self.reset_logger()\n",
    "        \n",
    "# env helpers\n",
    "def env_setup(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "    assert isinstance(env.action_space,Box), \"Sorry this VPG only works with continuous action spaces\"\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "\n",
    "    return env,obs_dim,act_dim\n",
    "\n",
    "def reset_env(env):\n",
    "    obs, rew, done, ep_rews, ep_vals = env.reset(), 0, False, [], []\n",
    "    return obs, rew, done, ep_rews, ep_vals\n",
    "\n",
    "\n",
    "\n",
    "def network(in_dim,out_dim,hidden_dim=32,activation=nn.Tanh,out_activation=None):\n",
    "    layers = [nn.Linear(in_dim, hidden_dim), activation(),\n",
    "              nn.Linear(hidden_dim, hidden_dim), activation(),\n",
    "              nn.Linear(hidden_dim, out_dim)]\n",
    "    if out_activation:\n",
    "        layers.append(out_activation())\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def log_likelihood(a,mu,std):\n",
    "    summand = (a-mu)**2/(std+EPS)**2 + 2*torch.log(std) + torch.log(2*torch.tensor(np.pi))\n",
    "    return -.5*torch.sum(summand,1)\n",
    "\n",
    "def gaussian_policy(obs,act,obs_dim,act_dim,hidden_dim=32,a=nn.Tanh,a_out=None):\n",
    "    # mu is a function\n",
    "    # pi is a function\n",
    "    # noise is a function\n",
    "    \n",
    "    mu_net = network(obs_dim,act_dim,hidden_dim,a,a_out)\n",
    "    mu = mu_net(obs)\n",
    "    log_std = torch.tensor(-0.5*np.ones(act_dim,dtype=np.float32))\n",
    "    std = torch.exp(log_std)\n",
    "    noise = torch.normal(tensor(np.zeros(mu.shape)),tensor(np.ones(mu.shape))).float()\n",
    "    pi = mu + noise * std\n",
    "    logp = log_likelihood(act,mu,std)\n",
    "    logpi = log_likelihood(pi,mu,std)\n",
    "    return pi, logp,logpi\n",
    "\n",
    "\n",
    "def actor_critic(obs,act,obs_dim,act_dim,hidden_dim=64,a=nn.Tanh,a_out=None):\n",
    "    pi, logp, _ =  gaussian_policy(obs, act, obs_dim,act_dim, hidden_dim, a, a_out)\n",
    "    vf = network(obs_dim,1,hidden_dim,a) # may need to squeeze\n",
    "    return pi, logp, vf\n",
    "\n",
    "def discount_cumsum(rews, gamma):\n",
    "    y = gamma**np.arange(len(rews))\n",
    "    gamma_mat=[np.roll(y, i, axis=0) for i in range(len(y))]\n",
    "    rews_mat = np.repeat([rews], [len(rews)], axis=0)\n",
    "    rews_mat = np.triu(rews_mat)*gamma_mat\n",
    "    return np.sum(rews_mat,axis=1)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self,obs_dim,act_dim,h_dim):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(obs_dim, h_dim)\n",
    "        self.layer2 = nn.Linear(h_dim, h_dim)\n",
    "        self.layer3 = nn.Linear(h_dim, act_dim)  # Prob of Left\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.layer1(x))\n",
    "        x = F.tanh(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "    def gaussian_policy(self,x):\n",
    "        mu = self.forward(x)\n",
    "        log_std = -0.5*torch.ones(act_dim)\n",
    "        std = tr.exp(log_std)\n",
    "        noise = tr.normal(torch.zeros(mu.shape),torch.ones(mu.shape))\n",
    "        pi = mu + noise * std\n",
    "        return pi,mu,std\n",
    "\n",
    "    def log_prob(self,obs,acts):\n",
    "        pi,mu,std = self.gaussian_policy(obs)\n",
    "        return log_likelihood(acts,mu,std)\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self,obs_dim,h_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(obs_dim, h_dim)\n",
    "        self.layer2 = nn.Linear(h_dim, h_dim)\n",
    "        self.layer3 = nn.Linear(h_dim, 1)  # Prob of Left\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.layer1(x))\n",
    "        x = F.tanh(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def gaussian_policy(x,actor):\n",
    "    mu = actor(x)\n",
    "    log_std = -0.5*torch.ones(act_dim)\n",
    "    std = torch.exp(log_std)\n",
    "    noise = torch.normal(tensor(np.zeros(mu.shape)),tensor(np.ones(mu.shape))).float()\n",
    "    pi = mu + noise * std\n",
    "    return pi,mu,std\n",
    "\n",
    "def log_prob(obs,acts,actor):\n",
    "    \n",
    "    pi,mu,std = gaussian_policy(obs,actor)\n",
    "    print(pi,mu,std)\n",
    "    return log_likelihood(acts,mu,std)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.7224, -0.6312, -1.9547, -0.4724, -0.4911, -1.3226, -0.6250, -0.8205,\n",
      "        -1.3231, -0.9611, -0.5322, -0.4250, -1.0231, -0.5980, -0.4985, -1.2943,\n",
      "        -1.0169, -0.5199, -1.3583, -1.7503, -1.3446, -0.4894, -1.0663, -1.5073,\n",
      "        -0.4410, -1.8719, -0.6539, -0.6794, -1.1414, -0.8046, -1.2949, -0.4379],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.9710, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor = Actor(obs_dim,act_dim,256)\n",
    "obs = tensor(np.random.random_sample((32,4))).float()\n",
    "acts = tensor(np.random.random_sample((32,1))).float()\n",
    "\n",
    "def gaussian_policy(x):\n",
    "    mu = actor(x)\n",
    "    log_std = -0.5*torch.ones(act_dim)\n",
    "    std = tr.exp(log_std)\n",
    "    noise = tr.normal(torch.zeros(mu.shape),torch.ones(mu.shape))\n",
    "    pi = mu + noise * std\n",
    "    return pi,mu,std\n",
    "\n",
    "def log_prob(obs,acts,actor):\n",
    "    \n",
    "    pi,mu,std = gaussian_policy(obs,actor)\n",
    "    print(pi,mu,std)\n",
    "    return log_likelihood(acts,mu,std)\n",
    "print(actor.log_prob(obs,acts))\n",
    "tr.mean(actor.log_prob(obs,acts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  pi_loss -0.036  v_loss 7.576  episode length 9.52  returns 9.52\n",
      "epoch 1  pi_loss 0.023  v_loss 5.311  episode length 7.72  returns 7.72\n",
      "epoch 2  pi_loss -0.009  v_loss 10.905  episode length 12.84  returns 12.84\n",
      "epoch 3  pi_loss 0.013  v_loss 9.910  episode length 13.2  returns 13.2\n",
      "epoch 4  pi_loss 0.008  v_loss 16.281  episode length 23.32  returns 23.32\n",
      "epoch 5  pi_loss -0.015  v_loss 18.351  episode length 27.58  returns 27.58\n",
      "epoch 6  pi_loss -0.008  v_loss 8.875  episode length 11.48  returns 11.48\n",
      "epoch 7  pi_loss -0.053  v_loss 28.667  episode length 48.17  returns 48.17\n",
      "epoch 8  pi_loss -0.025  v_loss 24.878  episode length 37.52  returns 37.52\n",
      "epoch 9  pi_loss 0.018  v_loss 25.836  episode length 40.85  returns 40.85\n",
      "epoch 10  pi_loss 0.012  v_loss 21.886  episode length 32.43  returns 32.43\n",
      "epoch 11  pi_loss 0.004  v_loss 19.498  episode length 27.14  returns 27.14\n",
      "epoch 12  pi_loss 0.006  v_loss 22.321  episode length 35.05  returns 35.05\n",
      "epoch 13  pi_loss -0.011  v_loss 27.575  episode length 46.76  returns 46.76\n",
      "epoch 14  pi_loss -0.006  v_loss 39.423  episode length 73.7  returns 73.7\n",
      "epoch 15  pi_loss -0.017  v_loss 41.810  episode length 91.85  returns 91.85\n",
      "epoch 16  pi_loss -0.012  v_loss 40.349  episode length 88.98  returns 88.98\n",
      "epoch 17  pi_loss -0.052  v_loss 33.184  episode length 63.49  returns 63.49\n",
      "epoch 18  pi_loss -0.041  v_loss 31.708  episode length 61.24  returns 61.24\n",
      "epoch 19  pi_loss -0.012  v_loss 40.323  episode length 92.8  returns 92.8\n",
      "epoch 20  pi_loss 0.007  v_loss 54.527  episode length 152.97  returns 152.97\n",
      "epoch 21  pi_loss -0.005  v_loss 45.884  episode length 109.96  returns 109.96\n",
      "epoch 22  pi_loss 0.014  v_loss 48.569  episode length 124.39  returns 124.39\n",
      "epoch 23  pi_loss -0.008  v_loss 53.990  episode length 151.15  returns 151.15\n",
      "epoch 24  pi_loss -0.015  v_loss 67.721  episode length 241.23  returns 241.23\n",
      "epoch 25  pi_loss -0.000  v_loss 71.023  episode length 279.44  returns 279.44\n",
      "epoch 26  pi_loss -0.039  v_loss 63.020  episode length 182.86  returns 182.86\n",
      "epoch 27  pi_loss -0.012  v_loss 76.347  episode length 338.53  returns 338.53\n",
      "epoch 28  pi_loss -0.011  v_loss 75.293  episode length 334.6  returns 334.6\n",
      "epoch 29  pi_loss -0.025  v_loss 80.253  episode length 445.58  returns 445.58\n",
      "epoch 30  pi_loss -0.011  v_loss 84.511  episode length 583.67  returns 583.67\n",
      "epoch 31  pi_loss -0.015  v_loss 90.215  episode length 1000.0  returns 1000.0\n",
      "epoch 32  pi_loss -0.001  v_loss 89.435  episode length 926.67  returns 926.67\n",
      "epoch 33  pi_loss 0.008  v_loss 89.945  episode length 1000.0  returns 1000.0\n",
      "epoch 34  pi_loss -0.002  v_loss 89.816  episode length 1000.0  returns 1000.0\n",
      "epoch 35  pi_loss 0.008  v_loss 89.685  episode length 1000.0  returns 1000.0\n",
      "epoch 36  pi_loss -0.014  v_loss 88.490  episode length 862.17  returns 862.17\n",
      "epoch 37  pi_loss -0.014  v_loss 89.476  episode length 1000.0  returns 1000.0\n",
      "epoch 38  pi_loss 0.002  v_loss 89.313  episode length 1000.0  returns 1000.0\n",
      "epoch 39  pi_loss -0.013  v_loss 89.093  episode length 1000.0  returns 1000.0\n",
      "epoch 40  pi_loss 0.020  v_loss 88.891  episode length 1000.0  returns 1000.0\n",
      "epoch 41  pi_loss -0.003  v_loss 88.724  episode length 1000.0  returns 1000.0\n",
      "epoch 42  pi_loss -0.016  v_loss 88.563  episode length 1000.0  returns 1000.0\n",
      "epoch 43  pi_loss -0.003  v_loss 88.406  episode length 1000.0  returns 1000.0\n",
      "epoch 44  pi_loss 0.010  v_loss 88.236  episode length 1000.0  returns 1000.0\n",
      "epoch 45  pi_loss -0.018  v_loss 88.067  episode length 1000.0  returns 1000.0\n",
      "epoch 46  pi_loss 0.003  v_loss 87.896  episode length 1000.0  returns 1000.0\n",
      "epoch 47  pi_loss -0.008  v_loss 87.491  episode length 838.17  returns 838.17\n",
      "epoch 48  pi_loss -0.016  v_loss 87.582  episode length 1000.0  returns 1000.0\n",
      "epoch 49  pi_loss 0.003  v_loss 87.437  episode length 1000.0  returns 1000.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as tr\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.tensor as tensor\n",
    "EPS = 1e-8\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "\n",
    "    def __init__(self):\n",
    "        # for evaluation at end of epoch\n",
    "        self.reset_epoch()\n",
    "        # for storing an episode\n",
    "        self.reset_episode()\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.ep_o, self.ep_a,self.ep_r,self.ep_v = [],[],[],[]\n",
    "        self.ep_l = 0\n",
    "\n",
    "    def reset_epoch(self):\n",
    "        self.obs_buf, self.acts_buf,self.advs_buf,self.rtgs_buf = [] ,[],[],[]\n",
    "        self.logp_prev = None\n",
    "\n",
    "\n",
    "    def store_batch(self,ep_obs,ep_acts,ep_advs,ep_rtgs):\n",
    "        # when episode is over, appends episode vals to batch\n",
    "        self.obs_buf += ep_obs\n",
    "        self.acts_buf += ep_acts\n",
    "        self.advs_buf += ep_advs\n",
    "        self.rtgs_buf += ep_rtgs\n",
    "\n",
    "    def get_batch(self):\n",
    "\n",
    "        b_a, b_o = np.array(self.acts_buf).reshape(-1), np.array(self.obs_buf)\n",
    "        # important: for continuous action space reshape acts to [batch_size,1]\n",
    "        b_a = b_a.reshape(-1,1)\n",
    "        # normalize trick\n",
    "        b_adv  = np.array((self.advs_buf - np.mean(self.advs_buf))/(np.std(self.advs_buf) + 1e-8))\n",
    "        b_rtg = np.array(self.rtgs_buf)\n",
    "\n",
    "        return [b_o,b_a,b_adv,b_rtg]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.obs_buf)\n",
    "\n",
    "    def store_episode(self,o,a,r,v):\n",
    "        self.ep_o.append(o)\n",
    "        self.ep_a.append(a)\n",
    "        self.ep_r.append(r)\n",
    "        self.ep_v.append(v)\n",
    "        self.ep_l+=1\n",
    "\n",
    "    def get_episode(self):\n",
    "        return self.ep_o,self.ep_a,self.ep_r,self.ep_v,self.ep_l\n",
    "\n",
    "    \n",
    "\n",
    "class Logger:\n",
    "    \"\"\"\n",
    "    Logs relevant values and prints them\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset_logger()\n",
    "\n",
    "    def reset_logger(self):\n",
    "        self.train_r, self.ep_len = [],[]\n",
    "\n",
    "    def store(self, train_r=None, ep_len=None, train=True):\n",
    "        if train:\n",
    "            self.train_r.append(train_r)\n",
    "            self.ep_len.append(ep_len)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def get_vals(self):\n",
    "\n",
    "        vals = np.round([np.mean(self.train_r),np.mean(self.ep_len)],2)\n",
    "        return vals\n",
    "\n",
    "    def print_epoch(self, epoch, loss1, loss2):\n",
    "        train_r, ep_len = self.get_vals()\n",
    "\n",
    "        print('epoch {0}  pi_loss {1:.3f}  v_loss {2:.3f}  episode length {3}  returns {4}'.format(epoch,loss1,loss2, ep_len,train_r ))\n",
    "        self.reset_logger()\n",
    "        \n",
    "# env helpers\n",
    "def env_setup(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "    assert isinstance(env.action_space,Box), \"Sorry this VPG only works with continuous action spaces\"\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "\n",
    "    return env,obs_dim,act_dim\n",
    "\n",
    "def reset_env(env):\n",
    "    obs, rew, done, ep_rews, ep_vals = env.reset(), 0, False, [], []\n",
    "    return obs, rew, done, ep_rews, ep_vals\n",
    "\n",
    "def log_likelihood(a,mu,std):\n",
    "    summand = (a-mu)**2/(std+EPS)**2 + 2*tr.log(std) + tr.log(2*tensor(np.pi))\n",
    "    return -.5*tr.sum(summand,1)\n",
    "\n",
    "def discount_cumsum(rews, gamma):\n",
    "    y = gamma**np.arange(len(rews))\n",
    "    gamma_mat=[np.roll(y, i, axis=0) for i in range(len(y))]\n",
    "    rews_mat = np.repeat([rews], [len(rews)], axis=0)\n",
    "    rews_mat = np.triu(rews_mat)*gamma_mat\n",
    "    return np.sum(rews_mat,axis=1)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self,obs_dim,act_dim,h_dim):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(obs_dim, h_dim)\n",
    "        self.layer2 = nn.Linear(h_dim, h_dim)\n",
    "        self.layer3 = nn.Linear(h_dim, act_dim)  # Prob of Left\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.layer1(x))\n",
    "        x = F.tanh(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "    def gaussian_policy(self,x):\n",
    "        mu = self.forward(x)\n",
    "        log_std = -0.5*torch.ones(act_dim)\n",
    "        std = tr.exp(log_std)\n",
    "        noise = tr.normal(torch.zeros(mu.shape),torch.ones(mu.shape))\n",
    "        pi = mu + noise * std\n",
    "        return pi,mu,std\n",
    "\n",
    "    def log_prob(self,obs,acts):\n",
    "        pi,mu,std = self.gaussian_policy(obs)\n",
    "        return log_likelihood(acts,mu,std)\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self,obs_dim,h_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(obs_dim, h_dim)\n",
    "        self.layer2 = nn.Linear(h_dim, h_dim)\n",
    "        self.layer3 = nn.Linear(h_dim, 1)  # Prob of Left\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.layer1(x))\n",
    "        x = F.tanh(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# main train loop\n",
    "def train(env_name='InvertedPendulum-v2',\n",
    "          pi_lr=1e-2,vf_lr=1e-3, gamma=0.99, lam=.95, n_iters=50, batch_size=5000\n",
    "          ):\n",
    "\n",
    "    env,obs_dim,act_dim = env_setup(env_name)\n",
    "    #adv_ph,ret_ph = placeholders(2) # replace these\n",
    "    pi = Actor(obs_dim,act_dim,256)\n",
    "    vf = Critic(obs_dim,64)\n",
    " \n",
    "    loss_pi = lambda obs,acts,adv: -torch.mean(log_probs(obs,acts,pi)*adv)\n",
    "    optimizer_pi = torch.optim.Adam(pi.parameters(),lr=pi_lr)\n",
    "    \n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    loss_v = lambda rets,obs: mse(rets,vf(obs))\n",
    "    optimizer_v = torch.optim.Adam(vf.parameters(),lr=vf_lr)\n",
    "    \n",
    "    def train_one_iteration(epoch):\n",
    "        # one epoch loop\n",
    "        buffer = Buffer()\n",
    "        logger = Logger()\n",
    "\n",
    "        obs, rew, done, ep_rews, ep_vals = reset_env(env)\n",
    "        while True:\n",
    "            # one episode loop\n",
    "\n",
    "            act = pi.gaussian_policy(tensor(obs).float())[0].detach().numpy() # TODO SET ACTION HERE USING PI #sess.run(pi, {obs_ph: obs.reshape(1,-1)})[0]\n",
    "            v_t = vf(tensor(obs).float())[0].detach().numpy() # TODO GET VALUE FUNC HERE USING ACTOR_CRITIC # sess.run(vf, {obs_ph: obs.reshape(1,-1)})[0]\n",
    "            \n",
    "         \n",
    "            obs2, rew, done, _ = env.step(act)\n",
    "            \n",
    "            \n",
    "            buffer.store_episode(obs,act,rew,v_t)\n",
    "\n",
    "            obs=obs2\n",
    "\n",
    "            if done:\n",
    "                # add episode to batch\n",
    "\n",
    "                # get episode from buffer\n",
    "                ep_obs, ep_acts, ep_rews, ep_vals, ep_len = buffer.get_episode() #len(ep_rews)\n",
    "\n",
    "                # run GAE to get advs\n",
    "                # outputs estimate for adv\n",
    "                # if agent died, last_value=reward\n",
    "                alive = ep_len == env._max_episode_steps\n",
    "                last_val = vf(tensor(obs).float()).detach().numpy()[0] if alive else rew\n",
    "\n",
    "                # start advantage compute\n",
    "                # add last value to compute TD \\gamma * V_{t+1} - V_{t}\n",
    "                ep_rews.append(last_val)\n",
    "                ep_vals.append(last_val)\n",
    "\n",
    "                # compute deltas for GAE\n",
    "                deltas=np.array(ep_rews[:-1]) + gamma * np.array(ep_vals[1:]) - np.array(ep_vals[:-1])\n",
    "\n",
    "                # go back to how it was\n",
    "                ep_rews = ep_rews[:-1]\n",
    "                ep_vals = ep_vals[:-1]\n",
    "\n",
    "                ep_advs = list(discount_cumsum(deltas, gamma*lam))\n",
    "                ep_rtgs = list(discount_cumsum(ep_rews, gamma))\n",
    "\n",
    "                buffer.store_batch(ep_obs, ep_acts,ep_advs,ep_rtgs)\n",
    "                logger.store(sum(ep_rews),len(ep_rews))\n",
    "\n",
    "                # reset episode\n",
    "\n",
    "                buffer.reset_episode()\n",
    "\n",
    "                obs, rew, done, ep_rews, ep_vals = reset_env(env)\n",
    "                if len(buffer) > batch_size:\n",
    "                    break\n",
    "        \n",
    "       \n",
    "        b_o, b_a, b_adv, b_rtg = buffer.get_batch()\n",
    "        b_o, b_a, b_adv, b_rtg = tensor(b_o).float(),tensor(b_a).float(),tensor(b_adv).float(),tensor(b_rtg).float()\n",
    "        \n",
    "        \n",
    "    \n",
    "        optimizer_pi.zero_grad()\n",
    "        # get log-likelihoods of state-action pairs\n",
    "        logp = pi.log_prob(b_o,b_a)\n",
    "        # choose loss to maximize likelihood*advantage\n",
    "        loss_pi = -tr.mean(logp*b_adv)\n",
    "        loss_pi.backward()\n",
    "        optimizer_pi.step()\n",
    "        \n",
    "        optimizer_v.zero_grad()\n",
    "        \n",
    "        \n",
    "        loss_v = tr.mean((b_rtg-vf(b_o).flatten())**2)\n",
    "        loss_v.backward()\n",
    "        optimizer_v.step()\n",
    "        logger.print_epoch(epoch,loss_pi.detach().numpy(),np.sqrt(loss_v.detach().numpy()))\n",
    "\n",
    "    for epoch in range(n_iters):\n",
    "        train_one_iteration(epoch)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs_dim = 4\n",
    "act_dim =1 \n",
    "obs = torch.randn(32, 4)\n",
    "a = torch.randn(32, 1)\n",
    "mu = network(obs_dim,act_dim,256)\n",
    "#mu(obs)\n",
    "#gaussian_policy(obs,a,obs_dim,act_dim,hidden_dim=32,a=nn.Tanh,a_out=None)\n",
    "x,y,z = actor_critic(obs,a,obs_dim,act_dim,64,a=nn.Tanh,a_out=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-4d24e5e879e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#pi = mu(obs) + torch.normal(tensor(np.zeros(mu.shape)),tensor(np.ones(mu,shape))) * std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/spinningup/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/spinningup/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/spinningup/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/spinningup/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/spinningup/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0m_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \"\"\"\n\u001b[0;32m-> 1350\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "network(obs_dim,act_dim)\n",
    "\n",
    "mu = network(obs_dim,act_dim)\n",
    "log_std = tensor(-0.5*np.ones(act_dim,dtype=np.float32))\n",
    "std = torch.exp(log_std)\n",
    "#pi = mu(obs) + torch.normal(tensor(np.zeros(mu.shape)),tensor(np.ones(mu,shape))) * std\n",
    "\n",
    "print(mu(obs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Double but got scalar type Float for argument #2 'mat2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-f97e0dbdedef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mreset_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgaussian_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobs_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-29bee5ce09c9>\u001b[0m in \u001b[0;36mgaussian_policy\u001b[0;34m(obs, act, obs_dim, act_dim, hidden_dim, a, a_out)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mlog_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mlogp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mlogpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/spinningup/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/spinningup/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/spinningup/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/spinningup/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/spinningup/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Double but got scalar type Float for argument #2 'mat2'"
     ]
    }
   ],
   "source": [
    "env,obs_dim,act_dim = env_setup('InvertedPendulum-v2')\n",
    "obs,_,_,_,_  = reset_env(env)\n",
    "\n",
    "gaussian_policy(tensor(obs),tensor([.1]),obs_dim,act_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tensor(np.ones((3,2)))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00204692, 0.00725553, 0.00259891, 0.00921754]), 0, False, [], [])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.action_space.shape\n",
    "env.observation_space.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tr\n",
    "# some labels\n",
    "labels = torch.arange(3)\n",
    "labels = labels.reshape(3, 1)\n",
    "\n",
    "num_classes = 4\n",
    "one_hot_target = (labels == torch.arange(num_classes).reshape(1, num_classes)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as tr\n",
    "a = tr.tensor([0,1,1,0,1,0])\n",
    "a = a.reshape(-1,1)\n",
    "act_dim = 2\n",
    "\n",
    "one_hot = (a == tr.arange(act_dim)).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.arange(act_dim).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6145, 0.2061, 1.0119, 0.4910, 1.2089])\n",
      "tensor([0.1710, 0.1137, 0.2544, 0.1511, 0.3098])\n",
      "tensor(1.)\n",
      "tensor([-1.7662, -2.1745, -1.3688, -1.8897, -1.1718])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Softmax is also in torch.nn.functional\n",
    "data = torch.randn(5)\n",
    "print(data)\n",
    "print(F.softmax(data, dim=0))\n",
    "print(F.softmax(data, dim=0).sum())  # Sums to 1 because it is a distribution!\n",
    "print(F.log_softmax(data, dim=0))  # theres also log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "m = Categorical(torch.tensor([[ 9, 0.25, 0.25, 0.25 ],[ 9, 0.25, 0.25, 0.25 ]]))\n",
    "m.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "for t in count():\n",
    "    print(t)\n",
    "\n",
    "    if t==10:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.,  6.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "x = Variable(torch.Tensor([0.1, 0.1]), requires_grad=True)\n",
    "f = 3 * x[0] ** 2 + 4 * x[0] * x[1] + x[1] **2\n",
    "grad_f, = autograd.grad(f, x, create_graph=True)\n",
    "z = grad_f @ v\n",
    "z.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0., 24.],\n",
      "        [48., 72.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "\n",
    "def nth_derivative(f, wrt, n):\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        grads = grad(f, wrt, create_graph=True)[0]\n",
    "        f = grads.sum()\n",
    "\n",
    "    return grads\n",
    "\n",
    "x = torch.arange(4, requires_grad=True,dtype=torch.float).reshape(2, 2)\n",
    "loss = (x ** 4).sum()\n",
    "\n",
    "print(nth_derivative(f=loss, wrt=x, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  pi_loss -2.026  v_loss 263.924  episode length 23.21  returns 23.21\n",
      "epoch 1  pi_loss -1.091  v_loss 84.591  episode length 14.19  returns 14.19\n",
      "epoch 2  pi_loss -0.710  v_loss 345.974  episode length 35.11  returns 35.11\n",
      "epoch 3  pi_loss -0.002  v_loss 585.976  episode length 60.67  returns 60.67\n",
      "epoch 4  pi_loss -0.713  v_loss 489.698  episode length 51.57  returns 51.57\n",
      "epoch 5  pi_loss 0.040  v_loss 884.475  episode length 108.0  returns 108.0\n",
      "epoch 6  pi_loss -0.040  v_loss 582.901  episode length 99.84  returns 99.84\n",
      "epoch 7  pi_loss -0.017  v_loss 1315.972  episode length 170.07  returns 170.07\n",
      "epoch 8  pi_loss -0.099  v_loss 405.469  episode length 106.15  returns 106.15\n",
      "epoch 9  pi_loss -0.313  v_loss 529.724  episode length 135.21  returns 135.21\n",
      "epoch 10  pi_loss -0.007  v_loss 2511.908  episode length 1000.0  returns 1000.0\n",
      "epoch 11  pi_loss -0.032  v_loss 1644.718  episode length 551.6  returns 551.6\n",
      "epoch 12  pi_loss -0.061  v_loss 882.640  episode length 284.61  returns 284.61\n",
      "epoch 13  pi_loss -0.002  v_loss 1175.307  episode length 1000.0  returns 1000.0\n",
      "epoch 14  pi_loss 0.005  v_loss 865.959  episode length 1000.0  returns 1000.0\n",
      "epoch 15  pi_loss -0.000  v_loss 635.148  episode length 1000.0  returns 1000.0\n",
      "epoch 16  pi_loss -0.012  v_loss 484.060  episode length 1000.0  returns 1000.0\n",
      "epoch 17  pi_loss -0.013  v_loss 408.147  episode length 1000.0  returns 1000.0\n",
      "epoch 18  pi_loss 0.000  v_loss 397.288  episode length 1000.0  returns 1000.0\n",
      "epoch 19  pi_loss -0.001  v_loss 436.573  episode length 1000.0  returns 1000.0\n",
      "epoch 20  pi_loss -0.050  v_loss 507.922  episode length 1000.0  returns 1000.0\n",
      "epoch 21  pi_loss 0.014  v_loss 592.465  episode length 1000.0  returns 1000.0\n",
      "epoch 22  pi_loss -0.025  v_loss 673.118  episode length 1000.0  returns 1000.0\n",
      "epoch 23  pi_loss -0.001  v_loss 861.378  episode length 838.0  returns 838.0\n",
      "epoch 24  pi_loss -0.026  v_loss 3695.609  episode length 167.8  returns 167.8\n",
      "epoch 25  pi_loss 0.001  v_loss 697.915  episode length 1000.0  returns 1000.0\n",
      "epoch 26  pi_loss -0.060  v_loss 966.514  episode length 660.88  returns 660.88\n",
      "epoch 27  pi_loss 0.005  v_loss 602.329  episode length 1000.0  returns 1000.0\n",
      "epoch 28  pi_loss -0.133  v_loss 668.510  episode length 786.29  returns 786.29\n",
      "epoch 29  pi_loss 0.008  v_loss 499.904  episode length 1000.0  returns 1000.0\n",
      "epoch 30  pi_loss 0.007  v_loss 458.412  episode length 1000.0  returns 1000.0\n",
      "epoch 31  pi_loss -0.031  v_loss 426.858  episode length 1000.0  returns 1000.0\n",
      "epoch 32  pi_loss -0.009  v_loss 406.179  episode length 1000.0  returns 1000.0\n",
      "epoch 33  pi_loss 0.008  v_loss 396.033  episode length 1000.0  returns 1000.0\n",
      "epoch 34  pi_loss -0.015  v_loss 477.859  episode length 781.29  returns 781.29\n",
      "epoch 35  pi_loss -0.078  v_loss 485.706  episode length 635.38  returns 635.38\n",
      "epoch 36  pi_loss -0.195  v_loss 413.794  episode length 1000.0  returns 1000.0\n",
      "epoch 37  pi_loss -0.010  v_loss 448.429  episode length 716.86  returns 716.86\n",
      "epoch 38  pi_loss -0.036  v_loss 440.334  episode length 1000.0  returns 1000.0\n",
      "epoch 39  pi_loss 0.029  v_loss 454.345  episode length 833.83  returns 833.83\n",
      "epoch 40  pi_loss -0.013  v_loss 812.979  episode length 123.05  returns 123.05\n",
      "epoch 41  pi_loss -0.016  v_loss 472.544  episode length 1000.0  returns 1000.0\n",
      "epoch 42  pi_loss -0.009  v_loss 475.142  episode length 1000.0  returns 1000.0\n",
      "epoch 43  pi_loss 0.007  v_loss 472.059  episode length 1000.0  returns 1000.0\n",
      "epoch 44  pi_loss -0.004  v_loss 464.176  episode length 1000.0  returns 1000.0\n",
      "epoch 45  pi_loss -0.028  v_loss 452.890  episode length 1000.0  returns 1000.0\n",
      "epoch 46  pi_loss -0.030  v_loss 439.848  episode length 1000.0  returns 1000.0\n",
      "epoch 47  pi_loss 0.004  v_loss 2342.710  episode length 101.74  returns 101.74\n",
      "epoch 48  pi_loss 0.011  v_loss 1757.055  episode length 128.49  returns 128.49\n",
      "epoch 49  pi_loss -0.017  v_loss 509.372  episode length 1000.0  returns 1000.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "tf.reset_default_graph()\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "# basic utils\n",
    "def placeholders(n):\n",
    "    assert (n>1),\"must provide more than 1 placeholder\"\n",
    "    return [tf.placeholder(shape=(None,), dtype=tf.float32) for i in range(n)]\n",
    "\n",
    "# env helpers\n",
    "def env_setup(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    obs_ph = tf.placeholder(shape=(None, obs_dim), dtype=tf.float32)\n",
    "\n",
    "    assert isinstance(env.action_space,Box), \"Sorry this PPO only works with continuous action spaces\"\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    act_ph = tf.placeholder(shape=(None,1), dtype=tf.float32)\n",
    "\n",
    "\n",
    "    return env,obs_dim,act_dim, obs_ph, act_ph\n",
    "\n",
    "def reset_env(env):\n",
    "    obs, rew, done, ep_rews, ep_vals = env.reset(), 0, False, [], []\n",
    "    return obs, rew, done, ep_rews, ep_vals\n",
    "\n",
    "# actor-critic construction\n",
    "def network(x,hidden_units=[32,32,1],a=tf.tanh,a_out=None):\n",
    "    for u in hidden_units[:-1]:\n",
    "        x = tf.layers.dense(x,units=u,activation=a)\n",
    "    return tf.layers.dense(x,units=hidden_units[-1],activation=a_out)\n",
    "\n",
    "def log_likelihood(a,mu,std):\n",
    "    summand = (a-mu)**2/(std+EPS)**2 + 2*tf.log(std) + tf.log(2*np.pi)\n",
    "    return -.5*tf.reduce_sum(summand,axis=1)\n",
    "\n",
    "def gaussian_policy(obs,act,act_dim,hidden_units=[32,32],a=tf.tanh,a_out=None):\n",
    "    mu = network(obs,hidden_units+[act_dim],a,a_out)\n",
    "    log_std = tf.get_variable(name='log_std',initializer=-0.5*np.ones(act_dim,dtype=np.float32))\n",
    "    std = tf.exp(log_std)\n",
    "    pi = mu + tf.random_normal(tf.shape(mu)) * std\n",
    "    logp = log_likelihood(act,mu,std)\n",
    "    logpi = log_likelihood(pi,mu,std)\n",
    "    return pi, logp,logpi\n",
    "\n",
    "def value_approx(obs,hidden_units=[64,64],a=tf.tanh):\n",
    "    return tf.squeeze(network(obs,hidden_units+[1],a),axis=1)\n",
    "\n",
    "def actor_critic(x,act,act_dim,hidden_units=[64,64],a=tf.tanh,a_out=None):\n",
    "    pi, logp, _ =  gaussian_policy(x, act, act_dim, hidden_units, a, a_out)\n",
    "    vf = value_approx(x,hidden_units=[64,64],a=tf.tanh)\n",
    "    return pi, logp, vf\n",
    "\n",
    "# reward helper\n",
    "def discount_cumsum(rews, gamma):\n",
    "    y = gamma**np.arange(len(rews))\n",
    "    gamma_mat=[np.roll(y, i, axis=0) for i in range(len(y))]\n",
    "    rews_mat = np.repeat([rews], [len(rews)], axis=0)\n",
    "    rews_mat = np.triu(rews_mat)*gamma_mat\n",
    "    return np.sum(rews_mat,axis=1)\n",
    "\n",
    "# Experience buffer and logger classes\n",
    "\n",
    "class Buffer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # for evaluation at end of epoch\n",
    "        self.reset_epoch()\n",
    "        # for storing an episode\n",
    "        self.reset_episode()\n",
    "        \n",
    "    def reset_episode(self):\n",
    "        self.ep_o, self.ep_a,self.ep_r,self.ep_v = [],[],[],[]\n",
    "        self.ep_l = 0\n",
    "    \n",
    "    def reset_epoch(self):\n",
    "        self.obs_buf, self.acts_buf,self.advs_buf,self.rtgs_buf = [] ,[],[],[]\n",
    "        self.logp_prev = None\n",
    "        \n",
    "        \n",
    "    def store_batch(self,ep_obs,ep_acts,ep_advs,ep_rtgs):\n",
    "        # when episode is over, appends episode vals to batch\n",
    "        self.obs_buf += ep_obs\n",
    "        self.acts_buf += ep_acts\n",
    "        self.advs_buf += ep_advs\n",
    "        self.rtgs_buf += ep_rtgs\n",
    "    \n",
    "    def get_batch(self):\n",
    "        \n",
    "        b_a, b_o = np.array(self.acts_buf).reshape(-1), np.array(self.obs_buf)\n",
    "        # important: for continuous action space reshape acts to [batch_size,1] \n",
    "        b_a = b_a.reshape(-1,1)\n",
    "        # normalize trick\n",
    "        b_adv  = np.array((self.advs_buf - np.mean(self.advs_buf))/(np.std(self.advs_buf) + 1e-8))\n",
    "        b_rtg = np.array(self.rtgs_buf)\n",
    "        \n",
    "        return [b_o,b_a,b_adv,b_rtg]\n",
    "    \n",
    "    def get_buffer_size(self):\n",
    "        return len(self.obs_buf)\n",
    "    \n",
    "    def store_episode(self,o,a,r,v):\n",
    "        self.ep_o.append(o)\n",
    "        self.ep_a.append(a)\n",
    "        self.ep_r.append(r)\n",
    "        self.ep_v.append(v)\n",
    "        self.ep_l+=1\n",
    "    \n",
    "    def get_episode(self):\n",
    "        return self.ep_o,self.ep_a,self.ep_r,self.ep_v,self.ep_l\n",
    "        \n",
    "class Logger:\n",
    "    \"\"\"\n",
    "    Logs relevant values and prints them \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset_logger()\n",
    "\n",
    "    def reset_logger(self):\n",
    "        self.train_r, self.ep_len = [],[]\n",
    "        \n",
    "    def store(self, train_r=None, ep_len=None, train=True):\n",
    "        if train:\n",
    "            self.train_r.append(train_r)\n",
    "            self.ep_len.append(ep_len)\n",
    "        else:\n",
    "            pass\n",
    "                \n",
    "    def get_vals(self):\n",
    "\n",
    "        vals = np.round([np.mean(self.train_r),np.mean(self.ep_len)],2)\n",
    "        return vals\n",
    "    \n",
    "\n",
    "    def print_epoch(self, epoch, loss1, loss2):\n",
    "        train_r, ep_len = self.get_vals()\n",
    "        \n",
    "        print('epoch {0}  pi_loss {1:.3f}  v_loss {2:.3f}  episode length {3}  returns {4}'.format(epoch,loss1,loss2, ep_len,train_r ))\n",
    "        self.reset_logger()\n",
    "\n",
    "# main train loop\n",
    "def train(env_name='InvertedPendulum-v2',\n",
    "          lr=3e-2,vf_lr=1e-1, gamma=0.99, lam=.95, n_iters=50, batch_size=5000\n",
    "          ):\n",
    "    \n",
    "    env,obs_dim,act_dim, obs_ph, act_ph = env_setup(env_name)\n",
    "    adv_ph,ret_ph = placeholders(2)\n",
    "    \n",
    "    pi, logp, vf = actor_critic(obs_ph, act_ph,act_dim=act_dim)\n",
    "    \n",
    "    loss_pi, loss_v = -tf.reduce_mean(logp*adv_ph), tf.reduce_mean((ret_ph-vf)**2)\n",
    "    train_pi = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss_pi)\n",
    "    train_v = tf.train.AdamOptimizer(learning_rate=vf_lr).minimize(loss_v)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def train_one_iteration(epoch):\n",
    "        # one epoch loop \n",
    "        buffer = Buffer()\n",
    "        logger = Logger()\n",
    "        \n",
    "        obs, rew, done, ep_rews, ep_vals = reset_env(env)\n",
    "        while True:\n",
    "            # one episode loop\n",
    "        \n",
    "            act = sess.run(pi, {obs_ph: obs.reshape(1,-1)})[0]\n",
    "            v_t = sess.run(vf, {obs_ph: obs.reshape(1,-1)})[0]\n",
    "            obs2, rew, done, _ = env.step(act)\n",
    "            \n",
    "            buffer.store_episode(obs,act,rew,v_t)\n",
    "            \n",
    "            obs=obs2\n",
    "            \n",
    "            if done:\n",
    "                # add episode to batch \n",
    "                \n",
    "                # get episode from buffer\n",
    "                ep_obs, ep_acts, ep_rews, ep_vals, ep_len = buffer.get_episode() #len(ep_rews)\n",
    "                \n",
    "                # run GAE to get advs\n",
    "                # outputs estimate for adv\n",
    "                # if agent died, last_value=reward\n",
    "                alive = ep_len == env._max_episode_steps\n",
    "                last_val = sess.run(vf, {obs_ph: obs.reshape(1,-1)})[0] if alive else rew \n",
    "\n",
    "                # start advantage compute\n",
    "                # add last value to compute TD \\gamma * V_{t+1} - V_{t}\n",
    "                ep_rews.append(last_val)\n",
    "                ep_vals.append(last_val)\n",
    "\n",
    "                # compute deltas for GAE\n",
    "                deltas=np.array(ep_rews[:-1]) + gamma * np.array(ep_vals[1:]) - np.array(ep_vals[:-1])  \n",
    "\n",
    "                # go back to how it was\n",
    "                ep_rews = ep_rews[:-1]\n",
    "                ep_vals = ep_vals[:-1]\n",
    "\n",
    "                ep_advs = list(discount_cumsum(deltas, gamma*lam))\n",
    "                ep_rtgs = list(discount_cumsum(ep_rews, gamma))\n",
    "\n",
    "                buffer.store_batch(ep_obs, ep_acts,ep_advs,ep_rtgs)\n",
    "                logger.store(sum(ep_rews),len(ep_rews))\n",
    "                \n",
    "                # reset episode\n",
    "\n",
    "                buffer.reset_episode()\n",
    "                \n",
    "                obs, rew, done, ep_rews, ep_vals = reset_env(env)\n",
    "                if (buffer.get_buffer_size() > batch_size):\n",
    "                    break\n",
    "        \n",
    "        b_o, b_a, b_adv, b_rtg = buffer.get_batch()\n",
    "\n",
    "        inputs = {obs_ph: b_o, act_ph: b_a, adv_ph: b_adv, ret_ph: b_rtg}     \n",
    "        \n",
    "        sess.run([train_pi,train_v], feed_dict=inputs)        \n",
    "        b_loss_pi, b_loss_v =  sess.run([loss_pi,loss_v], feed_dict=inputs)\n",
    "        \n",
    "        logger.print_epoch(epoch,b_loss_pi,b_loss_v)\n",
    "\n",
    "    for epoch in range(n_iters):\n",
    "        train_one_iteration(epoch)\n",
    "        \n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 4])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3,4])\n",
    "y = torch.tensor([2,2,1,5])\n",
    "torch.min(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03665843  0.03732891  0.01799848 -0.03420282]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "state = env.reset()\n",
    "print(state)\n",
    "min_pos = -2.0\n",
    "max_pos = 2.0\n",
    "symmetric_states = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.3       , -1.78888889, -1.27777778, -0.76666667, -0.25555556,\n",
       "        0.25555556,  0.76666667,  1.27777778,  1.78888889,  2.3       ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(min_pos,max_pos,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
